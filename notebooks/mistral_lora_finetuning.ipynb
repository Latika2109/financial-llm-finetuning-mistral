{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zel7HCFlbHOU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "login(token=\"your_token\")\n",
        "\n",
        "# **Step 1: Load Pretrained Mistral Model & Tokenizer**\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Define BitsAndBytesConfig for 4-bit quantization with CPU offloading\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True # Enable CPU offloading for 32-bit parts\n",
        ")\n",
        "\n",
        "# Load model with updated quantization config and device_map\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Let Transformers automatically handle device placement\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))  # Resize embeddings after adding pad_token\n",
        "\n",
        "# **Step 2: Load Merged Finance Dataset**\n",
        "def load_json_dataset(json_path):\n",
        "    with open(json_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "    return Dataset.from_list(data)\n",
        "\n",
        "dataset = load_json_dataset(\"/content/fixed_dataset.json\")\n",
        "\n",
        "# **Step 3: Tokenize Data**\n",
        "def tokenize_function(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    responses = examples[\"response\"]\n",
        "\n",
        "    # Ensure everything is a string before concatenation\n",
        "    instruction_strs = [instr if isinstance(instr, str) else \" \".join(instr) for instr in instructions]\n",
        "    response_strs = [resp if isinstance(resp, str) else \" \".join(resp) for resp in responses]\n",
        "\n",
        "    texts = [instr + \" \" + resp for instr, resp in zip(instruction_strs, response_strs)]\n",
        "\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "# Map function with batched=True\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"response\"])\n",
        "\n",
        "# **Step 4: Configure LoRA Parameters**\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # Rank (Adjust for memory/performance tradeoff)\n",
        "    lora_alpha=32,       # Scaling factor\n",
        "    lora_dropout=0.05,   # Dropout for regularization\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# **Step 5: Define Training Arguments**\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,  # Reduce batch size\n",
        "    gradient_accumulation_steps=8,  # Increase to compensate for batch size reduction\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Use bf16 if available\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # Otherwise, use fp16\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    output_dir=\"mistral_lora_finance\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "# **Step 6: Train the Model**\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# **Step 7: Save the Fine-Tuned LoRA Adapter**\n",
        "model.save_pretrained(\"mistral_lora_finance_adapter\")\n",
        "tokenizer.save_pretrained(\"mistral_lora_finance_adapter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3370ec4"
      },
      "source": [
        "# Research Document: Fine-tuning Mistral-7B for Financial Applications using LoRA\n",
        "\n",
        "## Abstract\n",
        "This document details the methodology for fine-tuning the `Mistral-7B-v0.1` large language model to a financial domain using the Low-Rank Adaptation (LoRA) technique. The objective is to adapt the pre-trained model's capabilities to better understand, generate, and respond to queries within a specialized financial context. The process leverages 4-bit quantization for efficient resource utilization and employs `trl`'s `SFTTrainer` for supervised fine-tuning on a custom JSON-formatted financial dataset. The outcome is a LoRA adapter that can be merged with the base model for deployment in financial NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f255d71a"
      },
      "source": [
        "## 1. Introduction\n",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks. However, their general-purpose nature often necessitates domain-specific adaptation for optimal performance in specialized fields. This research outlines a practical approach to fine-tuning the `Mistral-7B-v0.1` model, a powerful open-source LLM, for financial applications. The chosen fine-tuning strategy, LoRA, is selected for its computational efficiency, allowing for effective adaptation with limited resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad006a77"
      },
      "source": [
        "## 2. Methodology\n",
        "\n",
        "### 2.1 Model Selection and Quantization\n",
        "\n",
        "The base model selected for fine-tuning is `mistralai/Mistral-7B-v0.1`. This model is known for its strong performance and relatively smaller size compared to larger models, making it suitable for fine-tuning on consumer-grade hardware or cloud instances with limited GPU memory.\n",
        "\n",
        "To further optimize memory usage and computational speed, the model is loaded with **4-bit quantization** using `BitsAndBytesConfig`. This configuration includes:\n",
        "*   `load_in_4bit=True`: Enables 4-bit quantization.\n",
        "*   `bnb_4bit_compute_dtype=torch.float16`: Specifies `float16` as the compute data type for 4-bit tensors.\n",
        "*   `bnb_4bit_use_double_quant=True`: Activates double quantization, which quantizes the quantization constants, leading to further memory savings.\n",
        "*   `llm_int8_enable_fp32_cpu_offload=True`: Allows offloading of specific 32-bit components to the CPU, freeing up GPU memory.\n",
        "\n",
        "The model is loaded with `device_map=\"auto\"` to enable automatic device placement by the `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff3e335"
      },
      "source": [
        "### 2.2 Tokenizer Configuration\n",
        "\n",
        "The `AutoTokenizer` from the `transformers` library is used to load the tokenizer corresponding to the `Mistral-7B-v0.1` model. A crucial step involves ensuring a `pad_token` is defined. If the default tokenizer lacks one, a special token `[PAD]` is added, and the model's token embeddings are resized accordingly to accommodate this new token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "598a4967"
      },
      "source": [
        "### 2.3 Dataset Preparation\n",
        "\n",
        "**Dataset Source:** A custom financial dataset, expected to be located at `/content/fixed_dataset.json`, is loaded. The dataset is anticipated to be a JSON array where each element is an object containing `\"instruction\"` and `\"response\"` keys, representing question-answer pairs or prompts and their desired completions in a financial context.\n",
        "\n",
        "**Tokenization Function:** A `tokenize_function` is implemented to process the dataset. This function concatenates the `instruction` and `response` fields, ensuring they are string types, and then tokenizes the combined text. The tokenization process includes:\n",
        "*   `padding=\"max_length\"`: Pads sequences to the `max_length`.\n",
        "*   `truncation=True`: Truncates sequences longer than `max_length`.\n",
        "*   `max_length=512`: Sets the maximum sequence length for tokenization.\n",
        "\n",
        "The dataset is processed in batches to improve efficiency, and the original `instruction` and `response` columns are removed after tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ca100fb"
      },
      "source": [
        "### 2.4 LoRA Configuration\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is utilized to efficiently fine-tune the large model. LoRA injects trainable rank decomposition matrices into the transformer layers, significantly reducing the number of trainable parameters. The `LoraConfig` is set with the following parameters:\n",
        "*   `r=16`: The rank of the update matrices, controlling the expressiveness of the adaptation.\n",
        "*   `lora_alpha=32`: A scaling factor for the LoRA activations.\n",
        "*   `lora_dropout=0.05`: Dropout applied to the LoRA layers for regularization.\n",
        "*   `bias=\"none\"`: Specifies that bias parameters will not be trained.\n",
        "*   `task_type=\"CAUSAL_LM\"`: Defines the task as causal language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2227dd91"
      },
      "source": [
        "### 2.5 Training Arguments\n",
        "\n",
        "Training parameters are defined using `TrainingArguments` from the `transformers` library:\n",
        "*   `per_device_train_batch_size=1`: Small batch size due to memory constraints with large models.\n",
        "*   `gradient_accumulation_steps=8`: Compensates for the small batch size by accumulating gradients over multiple steps.\n",
        "*   `num_train_epochs=3`: The number of passes over the training dataset.\n",
        "*   `learning_rate=2e-4`: The initial learning rate for the optimizer.\n",
        "*   `weight_decay=0.01`: Regularization parameter.\n",
        "*   `bf16`/`fp16`: Mixed precision training is enabled, preferring `bf16` if supported by the hardware, otherwise defaulting to `fp16` for faster training and reduced memory footprint.\n",
        "*   `logging_steps=100`: Frequency of logging training metrics.\n",
        "*   `save_steps=500`: Frequency of saving model checkpoints.\n",
        "*   `output_dir=\"mistral_lora_finance\"`: Directory to save model outputs.\n",
        "*   `report_to=\"none\"`: Disables automatic reporting to external services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f374e55f"
      },
      "source": [
        "### 2.6 Trainer Initialization and Execution\n",
        "\n",
        "The `SFTTrainer` from the `trl` library is used to handle the supervised fine-tuning process. It is initialized with the LoRA-adapted model, the tokenized training dataset, and the defined training arguments. The `trainer.train()` method then commences the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f0281da"
      },
      "source": [
        "## 3. Results and Artifacts\n",
        "\n",
        "Upon successful completion of the training process, the fine-tuned LoRA adapter weights and the tokenizer configuration will be saved to the directory `mistral_lora_finance_adapter`. These artifacts can then be loaded to infer with the adapted model, potentially by merging the LoRA weights back into the base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3df1f990"
      },
      "source": [
        "## 4. Conclusion and Future Work\n",
        "\n",
        "This notebook provides a robust framework for domain-adapting large language models using efficient fine-tuning techniques. The fine-tuned Mistral-7B model is expected to exhibit enhanced performance on financial NLP tasks. Future work could involve evaluating the model on specific financial benchmarks, experimenting with different LoRA parameters, integrating more diverse financial datasets, or exploring alternative fine-tuning methods."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U trl\n",
        "# After this cell runs, please restart the Colab runtime (Runtime -> Restart runtime...) and then run all cells from the beginning."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFMtq8DOhEU9",
        "outputId": "462e5c55-a4a0-4326-9f58-745f906394b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.26.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
            "Downloading trl-0.26.0-py3-none-any.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.26.0\n"
          ]
        }
      ]
    }
  ]
}
